# Gaze Following in video with Speeches


## Overview
![](data/VGS_Architecture.png)
![](data/VGS_demo.gif)

The project proposes a new approach that considers gaze following with speeches, along with a new dataset (VideoGazeSpeech, i.e., VGS). The task of gaze following is to determine the gaze target of a person in an image by a given image or video frame. This approach integrates the correlation between auditory and visual cues and their joint contribution to the gaze following task for the first time and proposes a new multimodal ingenious architecture. Our project firstly uses SyncNet, a dual-stream ConvNet, to compute correlations between lips and speech. It will extract features from the original video to generate a comprehensive feature map containing the speaker's head position and the original frame. And then, the features will be fed into a Mask-RCNN neural network to detect gaze following targets. This project contributed the first audio-video dataset to the field of gaze following and has since pioneered the exploration of multimodal fusion in the field.


•	This research is the first exploration of multimodality for gaze tracking in deep learning.
•	The author proposes a multimodal architecture with the frame, audio and head position as input feature maps.
•	The VideoGazeSpeech(VGS) dataset proposed in this study is the first gaze tracking dataset that fuses audio and video.


## Folder Structure:
-KETI_VGS
    --configs
        ---Mask_RCNN
            ----requirement.txt
            ----environment.yaml
        ---Speaker_detector
            ----requirement.txt
            ----environment.yaml

 --data:
    Note: This folder you can find:
            (1) Sequence_Gaze_Frames: Sequence gaze frames information generated by VGS: csv file and frames. 
            (2) head_box: Head boxes data generated by VGS. 
            (3) GT_VGS_Frames: The whole VGS datasetVGS's GroundTruth in 2 differenct format(coco, videoattention).
            (4) Raw_Video: Raw videos (with audio) in VGS

    Details are as follows:

    ---(1) Sequence_Gaze_Frames
        Note: csv format:  [frame_name,head_x,head_y,gazetarget_xmin,gazetarget_ymin,gazetarget_xmax,gazetarget_ymax]
        frame_name format：imageID+FrameSeqID.jpg = 0742480.jpg

        ---------------------!!!!!!!!!!!!!!!!后期添加,要从重群电脑上合并一下到一个文件夹，然后上传到网盘 /home/zzq/yuqi_summer_project/coco-noHnoS.zip

    ---(2) head_box [here](https://drive.google.com/drive/folders/1gUFw-mG9kU_I9R1f-N_y1LlLGwmCThPR?usp=sharing)



    ---(3) GT_VGS_Frames
        Note: The frames with ground truth (gaze target labelling) in sequence for each videos in VGS dataset

        ----coco_GT_VGS_Frames
        Note: This is the VGS database formatted for coco, including ground truth bbox as input:
        eg: (image_name, xmin,ymin,xmax,ymax): (0271006.jpg,192,120,454,382)
        Dataset can be download in this link: 
        (Train/val/test dataset: coco-HS.zip, coco-H.zip, coco-noHnoS.zip
        GT label xml files: GT_label_all.zip):[here](https://drive.google.com/drive/folders/14txFPMU-yV1RyXhGXTGLJtj-urZD5CC2?usp=sharing)
        
        
        ----videoattentionFormat_GT_VGS_Frames
        Note: This is the VGS database formatted for VideoAttention Algorithms [ref](https://github.com/ejcgt/attention-target-detection), including ground truth bbox and head information as input: 
        eg: (image_name, xmin,ymin,xmax,ymax,head_x,head_y): (0271006.jpg,192,120,454,382,375,235)

        Dataset can be download in this link:[here](https://drive.google.com/drive/folders/1BEvRF7jWM0P_u_6rsND_mTOXao6l5gZ3?usp=share_link)

    ---(4) Raw_Video [here](https://drive.google.com/drive/folders/1g3scSCQrOKcbDFuPH83CTdQM8PR25o3o?usp=sharing)



 --utils
    ---Speaker_detector : Align multimodal (video + audio) to detect speaker and non-speaker, generate head box of speaker and non-speaker.
    ---Mask_RCNN : Detect gaze target boxes in the frame.
    

 --weights:
   Note: 
   Mask_RCNN's checkpoint is [here]
   Speaker_Detecter's checkpoint is [here]




Data_Preprocessing.py----Prepare data
MLP_Best_Gaze.py--generate final gaze point

README.md


## Contact
If you have any questions, please email Yuqi Hou at houyuqi21701@gmail.com
